{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations_with_replacement\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for cross-validation and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Sample_code_number', 'Clump_Thickness', 'Uniformity_of_Cell_Size', 'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei', 'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses','Class']\n",
    "data = pd.read_csv('breast-cancer-wisconsin.data', sep=',',header=None, names = names, na_values=['?'])\n",
    "#print(data.shape)\n",
    "#print(data.isnull().sum())\n",
    "data = data.dropna() # no NA in the df\n",
    "#print(data.shape)\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(data))\n",
    "n_training_samples = 400\n",
    "# split into train and test datasets \n",
    "learnset_data = data.iloc[indices[-n_training_samples:]]\n",
    "testset_data = data.iloc[indices[:-n_training_samples]]\n",
    "# get the data and labels in train and train datasets \n",
    "learnset_data_ds = learnset_data.drop(['Class'],axis=1)\n",
    "learnset_data_class = learnset_data['Class']\n",
    "learnset_data_ds_array = np.array(learnset_data_ds)\n",
    "learnset_data_class_array = np.array(learnset_data_class)\n",
    "# get the data and labels in test and test datasets \n",
    "testset_data_ds = testset_data.drop(['Class'],axis=1)\n",
    "testset_data_class = testset_data['Class']\n",
    "testset_data_ds_array = np.array(testset_data_ds)\n",
    "testset_data_class_array = np.array(testset_data_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the function of KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(instance1, instance2):\n",
    "    instance1 = np.array(instance1) \n",
    "    instance2 = np.array(instance2)  \n",
    "    return np.linalg.norm(instance1 - instance2)\n",
    "# find new points distance to the all points in the training set \n",
    "# it needs to compute the euclidean distance between the “new” observation and all the data points in the training set. It must then select the K nearest ones and perform a majority vote. It then assigns the corresponding label to the observation.\n",
    "def get_neighbors(training_set, \n",
    "                  labels, \n",
    "                  test_instance, \n",
    "                  k,\n",
    "                  distance):\n",
    "    distances = []\n",
    "    for index in range(len(training_set)):\n",
    "        dist = distance(test_instance, training_set[index])\n",
    "        distances.append((training_set[index], dist, labels[index]))\n",
    "    distances.sort(key=lambda x: x[1])# sort the distance for getting the neighbors\n",
    "    neighbors = distances[:k]\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc score for k=2 is 1.000\n",
      "acc score for k=3 is 0.975\n",
      "acc score for k=4 is 0.987\n",
      "acc score for k=5 is 0.972\n",
      "acc score for k=6 is 0.977\n",
      "acc score for k=7 is 0.972\n",
      "acc score for k=8 is 0.975\n",
      "acc score for k=17 is 0.967\n",
      "acc score for k=33 is 0.965\n"
     ]
    }
   ],
   "source": [
    "# Split the data into 10 parts\n",
    "split = np.array_split(learnset_data, 10)\n",
    "split_array = [np.array(x) for x in split]\n",
    "for k in list(range(2,9))+[17,33]:\n",
    "    ave_accuracy = []\n",
    "    for i in range(len(split)):\n",
    "        test_cv_data = split_array[i][:, 0:9]\n",
    "        test_cv_labels = split_array[i][:, 9]\n",
    "        train_cv_data = (np.concatenate(split_array[0:i] + split_array[i:10]))[:, 0:9]\n",
    "        train_cv_labels = (np.concatenate(split_array[0:i] + split_array[i:10]))[:, 9]\n",
    "        label_pred = []\n",
    "        for i in range(len(test_cv_data)):\n",
    "            #predictions = []\n",
    "            neighbors = get_neighbors(train_cv_data, train_cv_labels, test_cv_data[i],k,distance = distance)\n",
    "            raw = test_cv_labels[i].tolist()\n",
    "            #print(predictions0,predictions1,predictions2,testset_data_class_array[i]) \n",
    "            #ls = np.array([predictions0]+[predictions1]+[predictions2]+[raw])\n",
    "            ls = []\n",
    "            for j in neighbors:\n",
    "                ls.append(j[2])\n",
    "            #print(neighbors)\n",
    "            pred = list(set([i for i in ls if ls.count(i)>(k/2)]))\n",
    "            count_TN = [0]\n",
    "            count_FP = [0]\n",
    "            count_FN = [0]\n",
    "            count_TP = [0]\n",
    "            new_ls = pred + [raw] + count_TN + count_FP + count_FN + count_TP\n",
    "            #print(new_ls)\n",
    "            label_pred.append(new_ls)\n",
    "        # accuracy score\n",
    "        for i in range(len(label_pred)):\n",
    "            if (label_pred[i][0] == 2 and label_pred[i][1]==2):\n",
    "                label_pred[i][2] = 1  # TN, raw=2, pred=2\n",
    "            if (label_pred[i][0] == 2 and label_pred[i][1]==4):\n",
    "                label_pred[i][3] = 1  # FP, raw=2, pred=4\n",
    "            if (label_pred[i][0] == 4 and label_pred[i][1]==2):\n",
    "                label_pred[i][4] = 1  # FN, raw=4, pred=2\n",
    "            if (label_pred[i][0] == 4 and label_pred[i][1]==4):\n",
    "                label_pred[i][5] = 1  # TP, raw=4 , pred=4\n",
    "\n",
    "        df_pred = pd.DataFrame(label_pred)\n",
    "        #print(df_pred)\n",
    "        TN = df_pred.iloc[: ,2].sum()\n",
    "        #print(TN)\n",
    "        FP = df_pred.iloc[: ,3].sum()\n",
    "        FN = df_pred.iloc[: ,4].sum()\n",
    "        TP = df_pred.iloc[: ,5].sum()\n",
    "        # confusion matrix\n",
    "        # Accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "        Acuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "        # TPR (true positive rate, recall, or sensitivity) = TP / (TP + FN)\n",
    "        TPR = TP / (TP + FN)\n",
    "        # PPV (positive predictive value or precision) = TP / (TP + FP)\n",
    "        PPV = TP / (TP + FP)\n",
    "        # TNR (true negative rate or specificity) = TN / (TN + FP)\n",
    "        TNR = TN / (TN + FP)\n",
    "        # F1 Score = 2 × PPV × TPR / (PPV + TPR)\n",
    "        F1_Score = 2 * PPV * TPR / (PPV + TPR)\n",
    "        #print(Acuracy) #, TPR, PPV, TNR, F1_Score\n",
    "        #print(TN,FP,FN,TP)\n",
    "        ave_accuracy.append(Acuracy)\n",
    "    print('acc score for k=%d is %.3f'%(k,sum(ave_accuracy)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9702602230483272 0.9883720930232558 0.9239130434782609 0.9617486338797814 0.9550561797752809\n",
      "176 7 1 85.0\n"
     ]
    }
   ],
   "source": [
    "label_pred = []\n",
    "for i in range(len(testset_data_ds_array)):\n",
    "    #predictions = []\n",
    "    neighbors = get_neighbors(learnset_data_ds_array, learnset_data_class_array, testset_data_ds_array[i],2,distance = distance)\n",
    "    raw = testset_data_class_array[i].tolist()\n",
    "    #print(predictions0,predictions1,predictions2,testset_data_class_array[i]) \n",
    "    #ls = np.array([predictions0]+[predictions1]+[predictions2]+[raw])\n",
    "    ls = []\n",
    "    for j in neighbors:\n",
    "        ls.append(j[2])\n",
    "    #print(neighbors)\n",
    "    pred = list(set([i for i in ls if ls.count(i)>1]))\n",
    "    count_TN = [0]\n",
    "    count_FP = [0]\n",
    "    count_FN = [0]\n",
    "    count_TP = [0]\n",
    "    new_ls = pred + [raw] + count_TN + count_FP + count_FN + count_TP\n",
    "    #print(new_ls)\n",
    "    label_pred.append(new_ls)\n",
    "# accuracy score\n",
    "for i in range(len(label_pred)):\n",
    "    if (label_pred[i][0] == 2 and label_pred[i][1]==2):\n",
    "        label_pred[i][2] = 1  # TN, raw=2, pred=2\n",
    "    if (label_pred[i][0] == 2 and label_pred[i][1]==4):\n",
    "        label_pred[i][3] = 1  # FP, raw=2, pred=4\n",
    "    if (label_pred[i][0] == 4 and label_pred[i][1]==2):\n",
    "        label_pred[i][4] = 1  # FN, raw=4, pred=2\n",
    "    if (label_pred[i][0] == 4 and label_pred[i][1]==4):\n",
    "        label_pred[i][5] = 1  # TP, raw=4 , pred=4\n",
    "\n",
    "df_pred = pd.DataFrame(label_pred)\n",
    "#print(df_pred)\n",
    "TN = df_pred.iloc[: ,2].sum()\n",
    "#print(TN)\n",
    "FP = df_pred.iloc[: ,3].sum()\n",
    "FN = df_pred.iloc[: ,4].sum()\n",
    "TP = df_pred.iloc[: ,5].sum()\n",
    "# confusion matrix\n",
    "# Accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "Acuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "# TPR (true positive rate, recall, or sensitivity) = TP / (TP + FN)\n",
    "TPR = TP / (TP + FN)\n",
    "# PPV (positive predictive value or precision) = TP / (TP + FP)\n",
    "PPV = TP / (TP + FP)\n",
    "# TNR (true negative rate or specificity) = TN / (TN + FP)\n",
    "TNR = TN / (TN + FP)\n",
    "# F1 Score = 2 × PPV × TPR / (PPV + TPR)\n",
    "F1_Score = 2 * PPV * TPR / (PPV + TPR)\n",
    "print(Acuracy, TPR, PPV, TNR, F1_Score) \n",
    "print(TN,FP,FN,TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.permutation(20)\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(data, k_folds):\n",
    "    data_split = []\n",
    "    fold_size = int(len(data) / k_folds)\n",
    "    indices = np.random.permutation(len(data))\n",
    "    for i in range(k_folds):\n",
    "        data_split.append(data[indices[i*fold_size:(i+1)*fold_size]])\n",
    "    return data_split\n",
    "## impurity index\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(train_data, train_label):\n",
    "    p, n = sum(train_label==4), sum(train_label==2)\n",
    "    if p==0 or n==0:\n",
    "        impurity = 0\n",
    "    else:\n",
    "        impurity = 1 - (p/(p+n))**2 - (n/(p+n))**2\n",
    "    return impurity \n",
    "\n",
    "def entropy(train_data, train_label):\n",
    "    p, n = sum(train_label==4), sum(train_label==2)\n",
    "    if p==0 or n==0:\n",
    "        impurity = 0\n",
    "    else:\n",
    "        impurity = -1*(p/(p+n)*math.log(p/(p+n),2) + n/(p+n)*math.log(n/(p+n),2))\n",
    "    return impurity\n",
    "def misclassification_error(train_data, train_label):\n",
    "    p, n = sum(train_label==4), sum(train_label==2)\n",
    "    if p==0 or n==0:\n",
    "        return 0\n",
    "    else:\n",
    "        impurity = 1- max(p/(p+n), n/(p+n))\n",
    "    return impurity\n",
    "\n",
    "def decrease(train_data, train_label, metric, attr, thre):\n",
    "    x1, y1 = train_data[train_data[:, attr] > thre], train_label[train_data[:, attr] > thre]\n",
    "    x2, y2 = train_data[train_data[:, attr] < thre], train_label[train_data[:, attr] < thre]\n",
    "    \n",
    "    if metric == 'gini':\n",
    "        impurity = gini_index(train_data, train_label)\n",
    "        impurity_split = len(y1) / len(train_label) * gini_index(x1, y1) + len(y2) /len(train_label) * gini_index(x2, y2)\n",
    "        \n",
    "    elif metric == 'entropy':\n",
    "        impurity = entropy(train_data, train_label)\n",
    "        impurity_split = len(y1) / len(train_label) * entropy(x1, y1) + len(y2) /len(train_label) * entropy(x2, y2)\n",
    "        \n",
    "    else:\n",
    "        impurity = misclassification_error(train_data, train_label)\n",
    "        impurity_split = len(y1) / len(train_label) * misclassification_error(x1, y1) + len(y2) /len(train_label) * misclassification_error(x2, y2)\n",
    "\n",
    "    # return impurity decrease\n",
    "    return impurity - impurity_split\n",
    "## chose threshold\n",
    "\n",
    "def chose_thre(train_data, train_label, attr, metric):\n",
    "    values = set(train_data[:, attr])    # unique values\n",
    "    values_sort = sorted(list(values))\n",
    "    attr_thre = 0\n",
    "    max_im_dec = float('-inf')\n",
    "\n",
    "    # try all thresholds values \n",
    "    for i in range(len(values_sort)-1):\n",
    "        thre = (values_sort[i] + values_sort[i+1])/2\n",
    "        im_dec = decrease(train_data, train_label, metric, attr, thre)\n",
    "        if im_dec > max_im_dec:\n",
    "            max_im_dec = im_dec\n",
    "            attr_thre = thre\n",
    "    # find the best attr threshold\n",
    "    return attr_thre\n",
    "\n",
    "# select the best attr based on the informaiton gain and threshold\n",
    "def chose_attr(train_data, train_label, metric):\n",
    "    max_im_dec = float('-inf')\n",
    "    best_thre = None\n",
    "    best_attr = None\n",
    "    for attr in range(train_data.shape[1]):\n",
    "        thre = chose_thre(train_data, train_label, attr, metric)\n",
    "        im_dec  = decrease(train_data, train_label, metric, attr, thre)\n",
    "        if im_dec > max_im_dec:\n",
    "            max_im_dec = im_dec\n",
    "            best_attr = attr\n",
    "            best_thre = thre\n",
    "\n",
    "    return best_attr, best_thre\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, attribute, threshold):\n",
    "        self.attr = attribute\n",
    "        self.thre = threshold\n",
    "        self.height = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.leaf = False\n",
    "        self.predict = None\n",
    "        \n",
    "## generate tree\n",
    "def generate_tree(train_data, train_label, metric, max_depth, impurity_thre,depth=1):\n",
    "    P, N = sum(train_label==4), sum(train_label==2)\n",
    "    \n",
    "    if P==0 or N==0:\n",
    "        leaf = Node(None, None)\n",
    "        leaf.leaf = True\n",
    "        leaf.predict = 4 if P > N else 2\n",
    "        return leaf       \n",
    "    \n",
    "    # max_depth    \n",
    "    elif depth >= max_depth:\n",
    "        leaf = Node(None, None)\n",
    "        leaf.leaf = True\n",
    "        leaf.predict = 4 if P > N else 2\n",
    "        return leaf\n",
    "    else:\n",
    "        best_attr, best_thre = chose_attr(train_data, train_label, metric)\n",
    "        impurity = decrease(train_data, train_label, metric, best_attr, best_thre)\n",
    "\n",
    "        '''\n",
    "           if metric == 'gini':\n",
    "            best_attr, best_thre = chose_attr(train_data, train_label, metric)\n",
    "            impurity = decrease(train_data, train_label, 'gini', best_attr, best_thre)\n",
    "        elif metric == 'entropy':\n",
    "            best_attr, best_thre = chose_attr(train_data, train_label)\n",
    "            impurity = decrease(train_data, train_label, 'entropy', best_attr, best_thre)\n",
    "        else:\n",
    "            best_attr, best_thre = chose_attr(train_data, train_label)\n",
    "            impurity = decrease(train_data, train_label, 'misclassification_error', best_attr, best_thre)        \n",
    "\n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "    # impurity threshold\n",
    "    if impurity < impurity_thre:\n",
    "        # create a leaf none, since it is well separated\n",
    "        leaf = Node(None, None)\n",
    "        leaf.leaf = True\n",
    "        leaf.predict = 4 if P > N else 2\n",
    "        return leaf\n",
    "\n",
    "    else:\n",
    "        root = Node(best_attr, best_thre)\n",
    "\n",
    "        # split data \n",
    "        x1 = train_data[train_data[:, best_attr] < best_thre]\n",
    "        y1 = train_label[train_data[:, best_attr] < best_thre]\n",
    "        x2 = train_data[train_data[:, best_attr] > best_thre]\n",
    "        y2 = train_label[train_data[:, best_attr] > best_thre] \n",
    "    # recursively bulid the tree\n",
    "        root.left = generate_tree(x1, y1,metric, max_depth, impurity_thre, depth+1)\n",
    "        root.right = generate_tree(x2, y2,metric, max_depth, impurity_thre, depth+1)\n",
    "\n",
    "        return root\n",
    "def accuracy_score(pred_labels, labels):\n",
    "    TP = sum((pred_labels==4) & (labels==4))\n",
    "    TN = sum((pred_labels==2) & (labels==2))\n",
    "    return (TP + TN) / len(labels)\n",
    "\n",
    "## define a function to get the prediction for each obs\n",
    "def predict_row(node, row):\n",
    "    if node.leaf:\n",
    "        return node.predict\n",
    "    if row[node.attr] < node.thre:\n",
    "        return predict_row(node.left, row)\n",
    "    else:\n",
    "        return predict_row(node.right, row)\n",
    "    \n",
    "# predict testing data\n",
    "def predict(root, test_data):\n",
    "    predictions = []\n",
    "    for row in test_data:\n",
    "        pred = predict_row(root, row)\n",
    "        predictions.append(pred)\n",
    "    return np.array(predictions)\n",
    "# the height of a tree   \n",
    "def depth(tree):\n",
    "    depth_left = 0\n",
    "    depth_right = 0\n",
    "    if tree.left:\n",
    "        depth_left = depth(tree.left)\n",
    "    if tree.right:\n",
    "        depth_right = depth(tree.right)\n",
    "    if depth_left > depth_right:\n",
    "        return depth_left + 1\n",
    "    else:\n",
    "        return depth_right + 1\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, n_folds, metric, max_depth, impurity_thre):\n",
    "    X = cross_validation_split(dataset, n_folds)\n",
    "    scores = []\n",
    "    for i in range(len(X)):\n",
    "        test_cv_data = X[i][:, 0:-1]\n",
    "        test_cv_labels = X[i][:, -1]\n",
    "        train_cv_data = (np.concatenate(X[:i] + X[i:]))[:, 0:-1]\n",
    "        train_cv_labels = (np.concatenate(X[:i] + X[i:]))[:, -1]\n",
    "        \n",
    "        # run decision tree\n",
    "        tree = generate_tree(train_cv_data, train_cv_labels, metric, max_depth, impurity_thre,depth=1)\n",
    "        predictions = predict(tree, test_cv_data)\n",
    "        scores.append(accuracy_score(predictions, test_cv_labels))\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.933, TPR is 0.843, PPV is 0.966\n",
      "TNR is 0.983, F1_Score is 0.901\n",
      "178 3 16 86\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# max_depth = 10, impurity decrease is 0\n",
    "max_depth, impurity_thre = 10, 0\n",
    "tree = generate_tree(learnset_data_ds_array,learnset_data_class_array, 'entropy', max_depth, impurity_thre,depth=1)\n",
    "predicted_labels = predict(tree, testset_data_ds_array)\n",
    "labels = testset_data_class_array\n",
    "\n",
    "# performance metrics\n",
    "FP = sum((labels==2) & (predicted_labels==4))\n",
    "FN = sum((labels==4) & (predicted_labels==2))\n",
    "TP = sum((labels==4) & (predicted_labels==4))\n",
    "TN = sum((labels==2) & (predicted_labels==2))\n",
    "\n",
    "# confusion matrix\n",
    "# Accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "Acuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "# TPR (true positive rate, recall, or sensitivity) = TP / (TP + FN)\n",
    "TPR = TP / (TP + FN)\n",
    "# PPV (positive predictive value or precision) = TP / (TP + FP)\n",
    "PPV = TP / (TP + FP)\n",
    "# TNR (true negative rate or specificity) = TN / (TN + FP)\n",
    "TNR = TN / (TN + FP)\n",
    "# F1 Score = 2 × PPV × TPR / (PPV + TPR)\n",
    "F1_Score = 2 * PPV * TPR / (PPV + TPR)\n",
    "print('Accuracy is %.3f, TPR is %.3f, PPV is %.3f\\nTNR is %.3f, F1_Score is %.3f' %(Acuracy, TPR, PPV, TNR, F1_Score))\n",
    "print(TN,FP,FN,TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.919, TPR is 0.882, PPV is 0.891\n",
      "TNR is 0.939, F1_Score is 0.887\n",
      "170 11 12 90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# max_depth = 10, impurity decrease is 0\n",
    "max_depth, impurity_thre = 10, 0\n",
    "tree = generate_tree(learnset_data_ds_array,learnset_data_class_array, 'gini', max_depth, impurity_thre,depth=1)\n",
    "predicted_labels = predict(tree, testset_data_ds_array)\n",
    "labels = testset_data_class_array\n",
    "\n",
    "# performance metrics\n",
    "FP = sum((labels==2) & (predicted_labels==4))\n",
    "FN = sum((labels==4) & (predicted_labels==2))\n",
    "TP = sum((labels==4) & (predicted_labels==4))\n",
    "TN = sum((labels==2) & (predicted_labels==2))\n",
    "\n",
    "# confusion matrix\n",
    "# Accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "Acuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "# TPR (true positive rate, recall, or sensitivity) = TP / (TP + FN)\n",
    "TPR = TP / (TP + FN)\n",
    "# PPV (positive predictive value or precision) = TP / (TP + FP)\n",
    "PPV = TP / (TP + FP)\n",
    "# TNR (true negative rate or specificity) = TN / (TN + FP)\n",
    "TNR = TN / (TN + FP)\n",
    "# F1 Score = 2 × PPV × TPR / (PPV + TPR)\n",
    "F1_Score = 2 * PPV * TPR / (PPV + TPR)\n",
    "print('Accuracy is %.3f, TPR is %.3f, PPV is %.3f\\nTNR is %.3f, F1_Score is %.3f' %(Acuracy, TPR, PPV, TNR, F1_Score))\n",
    "print(TN,FP,FN,TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.940, TPR is 0.873, PPV is 0.957\n",
      "TNR is 0.978, F1_Score is 0.913\n",
      "177 4 13 89\n"
     ]
    }
   ],
   "source": [
    "max_depth, impurity_thre = 10, 0\n",
    "tree = generate_tree(learnset_data_ds_array,learnset_data_class_array, 'misclassification', max_depth, impurity_thre,depth=1)\n",
    "predicted_labels = predict(tree, testset_data_ds_array)\n",
    "labels = testset_data_class_array\n",
    "\n",
    "# performance metrics\n",
    "FP = sum((labels==2) & (predicted_labels==4))\n",
    "FN = sum((labels==4) & (predicted_labels==2))\n",
    "TP = sum((labels==4) & (predicted_labels==4))\n",
    "TN = sum((labels==2) & (predicted_labels==2))\n",
    "\n",
    "# confusion matrix\n",
    "# Accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "Acuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "# TPR (true positive rate, recall, or sensitivity) = TP / (TP + FN)\n",
    "TPR = TP / (TP + FN)\n",
    "# PPV (positive predictive value or precision) = TP / (TP + FP)\n",
    "PPV = TP / (TP + FP)\n",
    "# TNR (true negative rate or specificity) = TN / (TN + FP)\n",
    "TNR = TN / (TN + FP)\n",
    "# F1 Score = 2 × PPV × TPR / (PPV + TPR)\n",
    "F1_Score = 2 * PPV * TPR / (PPV + TPR)\n",
    "print('Accuracy is %.3f, TPR is %.3f, PPV is %.3f\\nTNR is %.3f, F1_Score is %.3f' %(Acuracy, TPR, PPV, TNR, F1_Score))\n",
    "print(TN,FP,FN,TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accu score for depth=2, impurity=0.000 is 0.970\n",
      "Accu score for depth=2, impurity=0.050 is 0.970\n",
      "Accu score for depth=2, impurity=0.100 is 0.970\n",
      "Accu score for depth=2, impurity=0.150 is 0.970\n",
      "Accu score for depth=2, impurity=0.200 is 0.970\n",
      "Accu score for depth=3, impurity=0.000 is 0.972\n",
      "Accu score for depth=3, impurity=0.050 is 0.970\n",
      "Accu score for depth=3, impurity=0.100 is 0.970\n",
      "Accu score for depth=3, impurity=0.150 is 0.970\n",
      "Accu score for depth=3, impurity=0.200 is 0.970\n",
      "Accu score for depth=4, impurity=0.000 is 0.975\n",
      "Accu score for depth=4, impurity=0.050 is 0.972\n",
      "Accu score for depth=4, impurity=0.100 is 0.970\n",
      "Accu score for depth=4, impurity=0.150 is 0.970\n",
      "Accu score for depth=4, impurity=0.200 is 0.970\n",
      "Accu score for depth=5, impurity=0.000 is 0.982\n",
      "Accu score for depth=5, impurity=0.050 is 0.980\n",
      "Accu score for depth=5, impurity=0.100 is 0.970\n",
      "Accu score for depth=5, impurity=0.150 is 0.970\n",
      "Accu score for depth=5, impurity=0.200 is 0.970\n",
      "Accu score for depth=6, impurity=0.000 is 0.988\n",
      "Accu score for depth=6, impurity=0.050 is 0.985\n",
      "Accu score for depth=6, impurity=0.100 is 0.970\n",
      "Accu score for depth=6, impurity=0.150 is 0.970\n",
      "Accu score for depth=6, impurity=0.200 is 0.970\n",
      "Accu score for depth=7, impurity=0.000 is 0.990\n",
      "Accu score for depth=7, impurity=0.050 is 0.988\n",
      "Accu score for depth=7, impurity=0.100 is 0.970\n",
      "Accu score for depth=7, impurity=0.150 is 0.970\n",
      "Accu score for depth=7, impurity=0.200 is 0.970\n",
      "Accu score for depth=8, impurity=0.000 is 0.995\n",
      "Accu score for depth=8, impurity=0.050 is 0.992\n",
      "Accu score for depth=8, impurity=0.100 is 0.970\n",
      "Accu score for depth=8, impurity=0.150 is 0.970\n",
      "Accu score for depth=8, impurity=0.200 is 0.970\n",
      "Accu score for depth=9, impurity=0.000 is 0.997\n",
      "Accu score for depth=9, impurity=0.050 is 0.995\n",
      "Accu score for depth=9, impurity=0.100 is 0.970\n",
      "Accu score for depth=9, impurity=0.150 is 0.970\n",
      "Accu score for depth=9, impurity=0.200 is 0.970\n",
      "Accu score for depth=10, impurity=0.000 is 1.000\n",
      "Accu score for depth=10, impurity=0.050 is 0.997\n",
      "Accu score for depth=10, impurity=0.100 is 0.970\n",
      "Accu score for depth=10, impurity=0.150 is 0.970\n",
      "Accu score for depth=10, impurity=0.200 is 0.970\n"
     ]
    }
   ],
   "source": [
    "X = np.array(learnset_data)[:,:-1]\n",
    "Y = np.array(learnset_data)[:,-1]\n",
    "X_mean = X-X.mean()\n",
    "u, s, vh = np.linalg.svd(X_mean)\n",
    "np.cumsum(s**2) / sum(s**2)  # 5 pcs\n",
    "X_pca = X_mean @ vh.T[:,:5]\n",
    "learnset_pca_data = np.column_stack([X_pca, Y])\n",
    "testset_pca_data = (testset_data_ds_array - testset_data_ds_array.mean()) @ vh.T[:,:5]\n",
    "### tune parameters\n",
    "score_pca_matrix = np.zeros((9,6))\n",
    "\n",
    "for d in range(2,11):  #max_depth\n",
    "    for i in [x/100 for x in range(0,25,5)]:\n",
    "        score = evaluate_algorithm(learnset_pca_data, 5, 'entropy', d, i)\n",
    "        score_pca_matrix[d-2, int(i*20)] = score\n",
    "        print('Accu score for depth=%d, impurity=%.3f is %.3f' %(d, i, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_pca_data = (testset_data_ds_array - testset_data_ds_array.mean()) @ vh.T[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The depth of tree is 10\n",
      "Accuracy is 0.951, TPR is 0.912, PPV is 0.949\n",
      "TNR is 0.972, F1_Score is 0.930\n",
      "176 5 9 93\n"
     ]
    }
   ],
   "source": [
    "max_depth, impurity_thre = 10, 0\n",
    "tree = generate_tree(learnset_pca_data[:, :5],learnset_data_class_array, 'entropy', max_depth, impurity_thre,depth=1)\n",
    "predicted_labels = predict(tree, testset_pca_data)\n",
    "labels = testset_data_class_array\n",
    "print('The depth of tree is %d' %depth(tree))\n",
    "\n",
    "# performance metrics\n",
    "FP = sum((labels==2) & (predicted_labels==4))\n",
    "FN = sum((labels==4) & (predicted_labels==2))\n",
    "TP = sum((labels==4) & (predicted_labels==4))\n",
    "TN = sum((labels==2) & (predicted_labels==2))\n",
    "\n",
    "# confusion matrix\n",
    "# Accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "Acuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "# TPR (true positive rate, recall, or sensitivity) = TP / (TP + FN)\n",
    "TPR = TP / (TP + FN)\n",
    "# PPV (positive predictive value or precision) = TP / (TP + FP)\n",
    "PPV = TP / (TP + FP)\n",
    "# TNR (true negative rate or specificity) = TN / (TN + FP)\n",
    "TNR = TN / (TN + FP)\n",
    "# F1 Score = 2 × PPV × TPR / (PPV + TPR)\n",
    "F1_Score = 2 * PPV * TPR / (PPV + TPR)\n",
    "print('Accuracy is %.3f, TPR is %.3f, PPV is %.3f\\nTNR is %.3f, F1_Score is %.3f' %(Acuracy, TPR, PPV, TNR, F1_Score))\n",
    "print(TN,FP,FN,TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_pca_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The depth of tree is 10\n",
      "Accuracy is 0.951, TPR is 0.892, PPV is 0.968\n",
      "TNR is 0.983, F1_Score is 0.929\n",
      "178 3 11 91\n"
     ]
    }
   ],
   "source": [
    "max_depth, impurity_thre = 10, 0\n",
    "tree = generate_tree(learnset_pca_data[:, :5],learnset_data_class_array, 'gini', max_depth, impurity_thre,depth=1)\n",
    "predicted_labels = predict(tree, testset_pca_data)\n",
    "labels = testset_data_class_array\n",
    "print('The depth of tree is %d' %depth(tree))\n",
    "\n",
    "# performance metrics\n",
    "FP = sum((labels==2) & (predicted_labels==4))\n",
    "FN = sum((labels==4) & (predicted_labels==2))\n",
    "TP = sum((labels==4) & (predicted_labels==4))\n",
    "TN = sum((labels==2) & (predicted_labels==2))\n",
    "\n",
    "# confusion matrix\n",
    "# Accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "Acuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "# TPR (true positive rate, recall, or sensitivity) = TP / (TP + FN)\n",
    "TPR = TP / (TP + FN)\n",
    "# PPV (positive predictive value or precision) = TP / (TP + FP)\n",
    "PPV = TP / (TP + FP)\n",
    "# TNR (true negative rate or specificity) = TN / (TN + FP)\n",
    "TNR = TN / (TN + FP)\n",
    "# F1 Score = 2 × PPV × TPR / (PPV + TPR)\n",
    "F1_Score = 2 * PPV * TPR / (PPV + TPR)\n",
    "print('Accuracy is %.3f, TPR is %.3f, PPV is %.3f\\nTNR is %.3f, F1_Score is %.3f' %(Acuracy, TPR, PPV, TNR, F1_Score))\n",
    "print(TN,FP,FN,TP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misclassification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The depth of tree is 10\n",
      "Accuracy is 0.972, TPR is 0.961, PPV is 0.961\n",
      "TNR is 0.978, F1_Score is 0.961\n",
      "177 4 4 98\n"
     ]
    }
   ],
   "source": [
    "max_depth, impurity_thre = 10, 0\n",
    "tree = generate_tree(learnset_pca_data[:, :5],learnset_data_class_array, 'misclassification', max_depth, impurity_thre,depth=1)\n",
    "predicted_labels = predict(tree, testset_pca_data)\n",
    "labels = testset_data_class_array\n",
    "print('The depth of tree is %d' %depth(tree))\n",
    "\n",
    "# performance metrics\n",
    "FP = sum((labels==2) & (predicted_labels==4))\n",
    "FN = sum((labels==4) & (predicted_labels==2))\n",
    "TP = sum((labels==4) & (predicted_labels==4))\n",
    "TN = sum((labels==2) & (predicted_labels==2))\n",
    "\n",
    "# confusion matrix\n",
    "# Accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "Acuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "# TPR (true positive rate, recall, or sensitivity) = TP / (TP + FN)\n",
    "TPR = TP / (TP + FN)\n",
    "# PPV (positive predictive value or precision) = TP / (TP + FP)\n",
    "PPV = TP / (TP + FP)\n",
    "# TNR (true negative rate or specificity) = TN / (TN + FP)\n",
    "TNR = TN / (TN + FP)\n",
    "# F1 Score = 2 × PPV × TPR / (PPV + TPR)\n",
    "F1_Score = 2 * PPV * TPR / (PPV + TPR)\n",
    "print('Accuracy is %.3f, TPR is %.3f, PPV is %.3f\\nTNR is %.3f, F1_Score is %.3f' %(Acuracy, TPR, PPV, TNR, F1_Score))\n",
    "print(TN,FP,FN,TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
